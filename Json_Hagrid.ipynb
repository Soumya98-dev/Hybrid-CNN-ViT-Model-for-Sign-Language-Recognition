{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa129a5e-d7fe-4c0c-83e4-a1452556b259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 JSON annotation files in /Users/soumyadeepchatterjee/Desktop/WayneState/Winter2025/FinalProject/hagrid/ann_subsample.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading JSON annotations: 100%|████████████████| 18/18 [00:00<00:00, 160.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded annotations for 1800 images.\n",
      "Starting processing images from: /Users/soumyadeepchatterjee/Desktop/WayneState/Winter2025/FinalProject/hagrid-sample-500k-384p/hagrid_500k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 509323 total image files to potentially process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|████████████| 509323/509323 [00:02<00:00, 185931.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing finished. Processed annotations for 1646 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "import traceback\n",
    "from tqdm import tqdm \n",
    "\n",
    "def calculate_bounding_box(image_width, image_height, landmarks, padding=10):\n",
    "    if not landmarks: return None\n",
    "    try:\n",
    "        x_coords = [lm[0] * image_width for lm in landmarks]\n",
    "        y_coords = [lm[1] * image_height for lm in landmarks]\n",
    "    except (TypeError, IndexError):\n",
    "        try:\n",
    "            x_coords = [lm.x * image_width for lm in landmarks]\n",
    "            y_coords = [lm.y * image_height for lm in landmarks]\n",
    "        except (AttributeError, TypeError):\n",
    "             print(\"Error: Unexpected landmark format. Cannot extract coordinates.\")\n",
    "             return None\n",
    "\n",
    "    if not x_coords or not y_coords: return None \n",
    "\n",
    "    x_min = int(min(x_coords) - padding)\n",
    "    y_min = int(min(y_coords) - padding)\n",
    "    x_max = int(max(x_coords) + padding)\n",
    "    y_max = int(max(y_coords) + padding)\n",
    "\n",
    "    x1 = max(0, x_min); y1 = max(0, y_min)\n",
    "    x2 = min(image_width, x_max); y2 = min(image_height, y_max)\n",
    "\n",
    "    if x1 >= x2 or y1 >= y2: return None \n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "annotation_dir = Path(\"/Users/soumyadeepchatterjee/Desktop/WayneState/Winter2025/FinalProject/hagrid/ann_subsample\")\n",
    "\n",
    "all_annotations = {}\n",
    "json_files = list(annotation_dir.glob(\"*.json\"))\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON annotation files in {annotation_dir}.\")\n",
    "\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(f\"No JSON files found in '{annotation_dir}'. Please check the path.\")\n",
    "\n",
    "for json_file in tqdm(json_files, desc=\"Loading JSON annotations\"):\n",
    "    try:\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            all_annotations.update(data) # Add annotations from this file\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {json_file}: {e}\")\n",
    "\n",
    "print(f\"Loaded annotations for {len(all_annotations)} images.\")\n",
    "\n",
    "if not all_annotations:\n",
    "     raise ValueError(\"Failed to load any annotations.\")\n",
    "\n",
    "base_image_dir = Path(\"/Users/soumyadeepchatterjee/Desktop/WayneState/Winter2025/FinalProject/hagrid-sample-500k-384p/hagrid_500k\")\n",
    "\n",
    "\n",
    "\n",
    "output_dir = Path(\"hand_crops_json_landmarks\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "PROCESS_LIMIT = 150000 \n",
    "processed_count = 0\n",
    "\n",
    "print(f\"Starting processing images from: {base_image_dir}\")\n",
    "\n",
    "\n",
    "image_files = list(base_image_dir.rglob(\"*.jpg\"))\n",
    "print(f\"Found {len(image_files)} total image files to potentially process.\")\n",
    "\n",
    "if not image_files:\n",
    "    raise FileNotFoundError(f\"No image files found in '{base_image_dir}' subdirectories. Please check the path.\")\n",
    "\n",
    "for image_path in tqdm(image_files, desc=\"Processing Images\"):\n",
    "    if PROCESS_LIMIT is not None and processed_count >= PROCESS_LIMIT:\n",
    "        print(f\"Reached processing limit ({PROCESS_LIMIT}). Stopping.\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        image_id = image_path.stem \n",
    "\n",
    "        if image_id in all_annotations:\n",
    "            annotation = all_annotations[image_id]\n",
    "            landmarks_list = annotation.get(\"landmarks\")\n",
    "            labels_list = annotation.get(\"labels\")\n",
    "\n",
    "            if not landmarks_list:\n",
    "                 continue\n",
    "\n",
    "            img = cv2.imread(str(image_path))\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not load image {image_path}. Skipping.\")\n",
    "                continue\n",
    "          \n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "            h_img, w_img, _ = img_rgb.shape\n",
    "\n",
    "            num_hands_processed_in_img = 0\n",
    "            for hand_idx, hand_landmarks in enumerate(landmarks_list):\n",
    "                if not isinstance(hand_landmarks, list) or not hand_landmarks or not all(isinstance(p, list) and len(p) >= 2 for p in hand_landmarks):\n",
    "                    continue\n",
    "\n",
    "                bbox = calculate_bounding_box(w_img, h_img, hand_landmarks, padding=5) # Using padding=5\n",
    "                if bbox is None:\n",
    "                    continue\n",
    "\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                hand_crop = img_rgb[y1:y2, x1:x2] \n",
    "                if hand_crop.size == 0: continue\n",
    "\n",
    "                hand_crop_resized = cv2.resize(hand_crop, (224, 224))\n",
    "\n",
    "                label = \"unknown\"\n",
    "                if labels_list and hand_idx < len(labels_list):\n",
    "                    label = labels_list[hand_idx]\n",
    "\n",
    "                out_path = output_dir / f\"{image_id}_hand{hand_idx}_{label}_lm.jpg\"\n",
    "                cv2.imwrite(str(out_path), cv2.cvtColor(hand_crop_resized, cv2.COLOR_RGB2BGR))\n",
    "                num_hands_processed_in_img += 1\n",
    "\n",
    "            if num_hands_processed_in_img > 0:\n",
    "                processed_count += 1\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing image {image_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"Processing finished. Processed annotations for {processed_count} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64191273-e3eb-4bc5-a0ed-1201d6950e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (YOLOv8)",
   "language": "python",
   "name": "yolov8_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
